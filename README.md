![alt text](BlackLivesMatter/static/blacklivesmatter-1.jpeg?raw=true)

### INTRODUCTION
Machine learning algorithms are amplifying present-day social inequities. Unfairness in such
algorithms are becoming increasingly popular and receiving attention from researchers to help
resolve the bias factor in applications such as criminal judgement, loan application, university
admission etc. The goal of this approach is to analyze a decision-making program and construct a
proof of its fairness. The focus is on equality of opportunity.
The prime focus of this research is to analyze the fairness of the algorithm based on the protected
and sensitive attributes such as race, region (demographic parity). Eventually, the motive behind
the project is to predict an outcome based on the given feature (primarily, race in this case). The
aim here is to remove discrimination by adjusting the predictor values, since the predictor, target
and outcome data are in the protected group. The fairness and accuracy of the algorithm are
eventually improved, which is the prime work done in the project.

### GOAL
Is to analyse a decision-making program and construct a proof of its fairness or unfairness.
The focus is on equality of opportunity.
The prime focus of this research is to analyse the fairness of the algorithm based on the protected
and sensitive attributes such as race, region (which comes under demographic parity). Aim here
is to remove discrimination by adjusting the predictor values, target and outcome data of the
protected group. The fairness and accuracy of the algorithm is eventually improved, which is the
prime work done in the project.

### CONCLUSION

*The shortcomings of the demographic parity as a fairness notion are removed by the introduction of equal opportunity and equalized odds. It is also fully aligned with the concept of supervised learning that is to build a higher accuracy classifier.​
*Although supervised learning have been quite successful which shows that this requirement is met in many applications, but it is not always possible to get labeled data.​
*For achieving better results with equalized odds we need features that are directly focused on target variable and not related to the protected attribute.​
*There is a need to go through data processing steps before passing it to the system for calculating thresholds. This shows we need domain level scrutiny.

### REFERENCES
Albarghouthi, A., D’Antoni, L., Drews, S., Nori, A. (November 2016). 
Fairness as a program property. In 3rd workshop on fairness, accountability and transparency in
machine learning. Retrieved from URL: https://arxiv.org/pdf/1610.06067.pdf
• Cythian dwork, M. H. E. A. (2011). Fairness through awareness. In cornell university. [3] et
al., C. D. (2003). It’s not privacy and it’s not fair. In standford law review.

• Et al., E. A. (2016a). Price of transparency in strategic machine learning. In 3rd workshop
on fairness, accountability and transparency in machine learning.
• Et al., Moritz. H. (2016b). Equality of opportunity in supervised learning. In 3rd workshop
on fairness, accountability and transparency in machine learning. Retrieved from URL:
https://arxiv.org/pdf/1610.02413.pdf
